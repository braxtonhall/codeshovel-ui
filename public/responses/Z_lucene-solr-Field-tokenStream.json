{
	"repo": "https://github.com/apache/lucene-solr.git",
	"file": "lucene/core/src/java/org/apache/lucene/document/Field.java",
	"method": {
		"longName": "public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse);",
		"startLine": 472,
		"methodName": "tokenStream",
		"isStatic": false,
		"isAbstract": false,
		"visibility": "public"
	},
	"history": {
		"105c7eae87896762cbcb295c73c8e8b1fd8f71f8": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-7413: move legacy numeric support to backwards module\n",
			"commitDate": "2016-08-17, 6:28 AM",
			"commitName": "105c7eae87896762cbcb295c73c8e8b1fd8f71f8",
			"commitAuthor": "Robert Muir",
			"commitDateOld": "2016-08-02, 2:10 AM",
			"commitNameOld": "d9df295bb73e011b72425d62ce609a14e4644aa4",
			"commitAuthorOld": "Mike McCandless",
			"daysBetweenCommits": 15.18,
			"commitsBetweenForRepo": 59,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,67 +1,38 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n     if (fieldType().indexOptions() == IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n-    final FieldType.LegacyNumericType numericType = fieldType().numericType();\n-    if (numericType != null) {\n-      if (!(reuse instanceof LegacyNumericTokenStream && ((LegacyNumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n-        // lazy init the TokenStream as it is heavy to instantiate\n-        // (attributes,...) if not needed (stored field loading)\n-        reuse = new LegacyNumericTokenStream(type.numericPrecisionStep());\n-      }\n-      final LegacyNumericTokenStream nts = (LegacyNumericTokenStream) reuse;\n-      // initialize value in TokenStream\n-      final Number val = (Number) fieldsData;\n-      switch (numericType) {\n-      case INT:\n-        nts.setIntValue(val.intValue());\n-        break;\n-      case LONG:\n-        nts.setLongValue(val.longValue());\n-        break;\n-      case FLOAT:\n-        nts.setFloatValue(val.floatValue());\n-        break;\n-      case DOUBLE:\n-        nts.setDoubleValue(val.doubleValue());\n-        break;\n-      default:\n-        throw new AssertionError(\"Should never get here\");\n-      }\n-      return reuse;\n-    }\n-\n     if (!fieldType().tokenized()) {\n       if (stringValue() != null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() != null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"75dd5e9f9e13c72890f1e5b1695f8281fe990d94": {
			"type": "Yexceptionschange",
			"commitMessage": "LUCENE-6988: IndexableField.tokenStream() no longer throws IOException\n",
			"commitDate": "2016-01-25, 2:04 AM",
			"commitName": "75dd5e9f9e13c72890f1e5b1695f8281fe990d94",
			"commitAuthor": "Alan Woodward",
			"commitDateOld": "2016-01-17, 11:54 AM",
			"commitNameOld": "24c46305bd8f335c3d0e501a33dd3da82732c49e",
			"commitAuthorOld": "Michael McCandless",
			"daysBetweenCommits": 7.59,
			"commitsBetweenForRepo": 69,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,67 +1,67 @@\n-  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n     if (fieldType().indexOptions() == IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final FieldType.LegacyNumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(reuse instanceof LegacyNumericTokenStream && ((LegacyNumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new LegacyNumericTokenStream(type.numericPrecisionStep());\n       }\n       final LegacyNumericTokenStream nts = (LegacyNumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() != null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() != null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {
				"oldValue": "[IOException]",
				"newValue": "[]"
			}
		},
		"7da175b0b6b4185ee6b5df852e59b93d9a9a1c86": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-6917: rename/deprecate numeric classes in favor of dimensional values\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1719562 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2015-12-11, 1:13 PM",
			"commitName": "7da175b0b6b4185ee6b5df852e59b93d9a9a1c86",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2015-07-02, 7:09 AM",
			"commitNameOld": "1816ed194556a8199c0338272d1c894036aa163f",
			"commitAuthorOld": "Uwe Schindler",
			"daysBetweenCommits": 162.29,
			"commitsBetweenForRepo": 875,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,67 +1,67 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (fieldType().indexOptions() == IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n-    final NumericType numericType = fieldType().numericType();\n+    final FieldType.LegacyNumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n-      if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n+      if (!(reuse instanceof LegacyNumericTokenStream && ((LegacyNumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        reuse = new NumericTokenStream(type.numericPrecisionStep());\n+        reuse = new LegacyNumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts = (NumericTokenStream) reuse;\n+      final LegacyNumericTokenStream nts = (LegacyNumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() != null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() != null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse = new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"249d0d25fec0c8d3aeaa8991b22c96317b6db86a": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-5989: allow passing BytesRef to StringField to make it easier to index arbitrary binary tokens\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1672781 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2015-04-10, 3:24 PM",
			"commitName": "249d0d25fec0c8d3aeaa8991b22c96317b6db86a",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2015-02-07, 2:10 AM",
			"commitNameOld": "376256316b016f4971bfa86517c750fae42158c7",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 62.51,
			"commitsBetweenForRepo": 511,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,58 +1,67 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (fieldType().indexOptions() == IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n-      if (stringValue() == null) {\n+      if (stringValue() != null) {\n+        if (!(reuse instanceof StringTokenStream)) {\n+          // lazy init the TokenStream as it is heavy to instantiate\n+          // (attributes,...) if not needed\n+          reuse = new StringTokenStream();\n+        }\n+        ((StringTokenStream) reuse).setValue(stringValue());\n+        return reuse;\n+      } else if (binaryValue() != null) {\n+        if (!(reuse instanceof BinaryTokenStream)) {\n+          // lazy init the TokenStream as it is heavy to instantiate\n+          // (attributes,...) if not needed\n+          reuse = new BinaryTokenStream();\n+        }\n+        ((BinaryTokenStream) reuse).setValue(binaryValue());\n+        return reuse;\n+      } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(reuse instanceof StringTokenStream)) {\n-        // lazy init the TokenStream as it is heavy to instantiate\n-        // (attributes,...) if not needed (stored field loading)\n-        reuse = new StringTokenStream();\n-      }\n-      ((StringTokenStream) reuse).setValue(stringValue());\n-      return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"6bf44e94399e474ba3285d442ce6406cdadc1d9e": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-6039: NO -> NONE\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1635861 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2014-10-31, 1:48 PM",
			"commitName": "6bf44e94399e474ba3285d442ce6406cdadc1d9e",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2014-10-31, 8:10 AM",
			"commitNameOld": "bc41d58cd37ab38c1a088ea67197bd3c338ac53f",
			"commitAuthorOld": "Michael McCandless",
			"daysBetweenCommits": 0.23,
			"commitsBetweenForRepo": 5,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,58 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (fieldType().indexOptions() == IndexOptions.NO) {\n+    if (fieldType().indexOptions() == IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"bc41d58cd37ab38c1a088ea67197bd3c338ac53f": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-6039: cutover to IndexOptions.NO/DocValuesType.NO instead of null\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1635790 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2014-10-31, 8:10 AM",
			"commitName": "bc41d58cd37ab38c1a088ea67197bd3c338ac53f",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2014-10-21, 12:32 AM",
			"commitNameOld": "8f9f8a3252c73428e67bc5d390e58d1370e060ba",
			"commitAuthorOld": "Michael McCandless",
			"daysBetweenCommits": 10.32,
			"commitsBetweenForRepo": 79,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,58 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (fieldType().indexOptions() == null) {\n+    if (fieldType().indexOptions() == IndexOptions.NO) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"8f9f8a3252c73428e67bc5d390e58d1370e060ba": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-6013: remove IndexableFieldType.indexed and FieldInfo.indexed (it's redundant with IndexOptions != null)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1633296 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2014-10-21, 12:32 AM",
			"commitName": "8f9f8a3252c73428e67bc5d390e58d1370e060ba",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2014-05-09, 9:36 AM",
			"commitNameOld": "e12039a377c4639f30aad8b31fb39964754d6084",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 164.62,
			"commitsBetweenForRepo": 1136,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,57 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (!fieldType().indexed()) {\n+    if (fieldType().indexOptions() == null) {\n+      // Not indexed\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse = new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"1613f1882c00f28f12570e4f75f913a663e1e2c0": {
			"type": "Ymultichange(Yparameterchange,Ybodychange)",
			"commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2014-05-02, 11:07 AM",
			"commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
			"commitAuthor": "Robert Muir",
			"subchanges": [
				{
					"type": "Yparameterchange",
					"commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2014-05-02, 11:07 AM",
					"commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
					"commitAuthor": "Robert Muir",
					"commitDateOld": "2014-04-29, 10:18 AM",
					"commitNameOld": "f5de5d01e8108653b2d6e2dcb0f7e9af7389937a",
					"commitAuthorOld": "",
					"daysBetweenCommits": 3.03,
					"commitsBetweenForRepo": 36,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,57 +1,57 @@\n-  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n-      if (!(internalTokenStream instanceof NumericTokenStream)) {\n+      if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n+        reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n+      final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n-      return internalTokenStream;\n+      return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(internalTokenStream instanceof StringTokenStream)) {\n+      if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream = new StringTokenStream();\n+        reuse = new StringTokenStream();\n       }\n-      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n-      return internalTokenStream;\n+      ((StringTokenStream) reuse).setValue(stringValue());\n+      return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {
						"oldValue": "[analyzer-Analyzer]",
						"newValue": "[analyzer-Analyzer, reuse-TokenStream]"
					}
				},
				{
					"type": "Ybodychange",
					"commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2014-05-02, 11:07 AM",
					"commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
					"commitAuthor": "Robert Muir",
					"commitDateOld": "2014-04-29, 10:18 AM",
					"commitNameOld": "f5de5d01e8108653b2d6e2dcb0f7e9af7389937a",
					"commitAuthorOld": "",
					"daysBetweenCommits": 3.03,
					"commitsBetweenForRepo": 36,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,57 +1,57 @@\n-  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n-      if (!(internalTokenStream instanceof NumericTokenStream)) {\n+      if (!(reuse instanceof NumericTokenStream && ((NumericTokenStream)reuse).getPrecisionStep() == type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n+        reuse = new NumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n+      final NumericTokenStream nts = (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n-      return internalTokenStream;\n+      return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(internalTokenStream instanceof StringTokenStream)) {\n+      if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream = new StringTokenStream();\n+        reuse = new StringTokenStream();\n       }\n-      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n-      return internalTokenStream;\n+      ((StringTokenStream) reuse).setValue(stringValue());\n+      return reuse;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {}
				}
			]
		},
		"43974d668667ba1b1dacf26a18a22c7fea909539": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-5339: javadocs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1554710 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2014-01-01, 4:23 PM",
			"commitName": "43974d668667ba1b1dacf26a18a22c7fea909539",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2013-11-18, 6:53 AM",
			"commitNameOld": "18117c0b04620e0e4bb7403fca5d05d35665de08",
			"commitAuthorOld": "Michael McCandless",
			"daysBetweenCommits": 44.4,
			"commitsBetweenForRepo": 225,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,57 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; this=\" + this);\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"18117c0b04620e0e4bb7403fca5d05d35665de08": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-5339: assocations\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1543047 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2013-11-18, 6:53 AM",
			"commitName": "18117c0b04620e0e4bb7403fca5d05d35665de08",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2013-07-08, 10:55 AM",
			"commitNameOld": "f092795fe94ba727f7368b63d8eb1ecd39749fc4",
			"commitAuthorOld": "Uwe Schindler",
			"daysBetweenCommits": 132.87,
			"commitsBetweenForRepo": 835,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,57 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; this=\" + this);\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"f092795fe94ba727f7368b63d8eb1ecd39749fc4": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-5097: Analyzer now has an additional tokenStream(String fieldName, String text) method, so wrapping by StringReader for common use is no longer needed. This method uses an internal reuseable reader, which was previously only used by the Field class.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500862 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2013-07-08, 10:55 AM",
			"commitName": "f092795fe94ba727f7368b63d8eb1ecd39749fc4",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2013-01-21, 10:36 PM",
			"commitNameOld": "06bf9a0857e997e7c5251ca8546556d1cdadf80f",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 167.47,
			"commitsBetweenForRepo": 1345,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,61 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n-      if (internalReader == null) {\n-        internalReader = new ReusableStringReader();\n-      }\n-      internalReader.setValue(stringValue());\n-      return analyzer.tokenStream(name(), internalReader);\n+      return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"8f88aa64978a61125adafff544c8e5084d497fb5": {
			"type": "Ybodychange",
			"commitMessage": "Turn some \"assert false\" in switch and switch-like statements into AssertionErrors. If we get into the default block we are wrong and we can also throw Ex, because then there is logic error (e.g. after new enum constant was added)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1379450 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-08-31, 6:55 AM",
			"commitName": "8f88aa64978a61125adafff544c8e5084d497fb5",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2012-08-30, 7:33 AM",
			"commitNameOld": "a4702d3711e8ad2ac5ef455e4182f2da80011c86",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 0.97,
			"commitsBetweenForRepo": 17,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,61 +1,61 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n-        assert false : \"Should never get here\";\n+        throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       if (internalReader == null) {\n         internalReader = new ReusableStringReader();\n       }\n       internalReader.setValue(stringValue());\n       return analyzer.tokenStream(name(), internalReader);\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"518fc20d1cf385650202ff9a3b35136ed9d646e5": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-4315: Add ReusableStringReader to Field.java\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1376261 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-08-22, 2:29 PM",
			"commitName": "518fc20d1cf385650202ff9a3b35136ed9d646e5",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2012-08-21, 5:43 AM",
			"commitNameOld": "e53aee7739be1c04bd1673a55b4956efb63c337f",
			"commitAuthorOld": "Uwe Schindler",
			"daysBetweenCommits": 1.37,
			"commitsBetweenForRepo": 8,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,57 +1,61 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val = (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         assert false : \"Should never get here\";\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream = new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n-      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n+      if (internalReader == null) {\n+        internalReader = new ReusableStringReader();\n+      }\n+      internalReader.setValue(stringValue());\n+      return analyzer.tokenStream(name(), internalReader);\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"e53aee7739be1c04bd1673a55b4956efb63c337f": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-4317: Improve reuse of internal TokenStreams in oal.document.Field.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1375507 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-08-21, 5:43 AM",
			"commitName": "e53aee7739be1c04bd1673a55b4956efb63c337f",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2012-08-03, 1:26 PM",
			"commitNameOld": "8f726e254bc060d45590f72411db69c943e1216b",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 17.68,
			"commitsBetweenForRepo": 211,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,75 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n-      if (numericTokenStream == null) {\n+      if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n-        // initialize value in TokenStream\n-        final Number val = (Number) fieldsData;\n-        switch (numericType) {\n-        case INT:\n-          numericTokenStream.setIntValue(val.intValue());\n-          break;\n-        case LONG:\n-          numericTokenStream.setLongValue(val.longValue());\n-          break;\n-        case FLOAT:\n-          numericTokenStream.setFloatValue(val.floatValue());\n-          break;\n-        case DOUBLE:\n-          numericTokenStream.setDoubleValue(val.doubleValue());\n-          break;\n-        default:\n-          assert false : \"Should never get here\";\n-        }\n-      } else {\n-        // OK -- previously cached and we already updated if\n-        // setters were called.\n+        internalTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n       }\n-\n-      return numericTokenStream;\n+      final NumericTokenStream nts = (NumericTokenStream) internalTokenStream;\n+      // initialize value in TokenStream\n+      final Number val = (Number) fieldsData;\n+      switch (numericType) {\n+      case INT:\n+        nts.setIntValue(val.intValue());\n+        break;\n+      case LONG:\n+        nts.setLongValue(val.longValue());\n+        break;\n+      case FLOAT:\n+        nts.setFloatValue(val.floatValue());\n+        break;\n+      case DOUBLE:\n+        nts.setDoubleValue(val.doubleValue());\n+        break;\n+      default:\n+        assert false : \"Should never get here\";\n+      }\n+      return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-\n-      return new TokenStream() {\n-        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n-        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n-        boolean used;\n-\n-        @Override\n-        public boolean incrementToken() {\n-          if (used) {\n-            return false;\n-          }\n-          termAttribute.setEmpty().append(stringValue());\n-          offsetAttribute.setOffset(0, stringValue().length());\n-          used = true;\n-          return true;\n-        }\n-\n-        @Override\n-        public void reset() {\n-          used = false;\n-        }\n-      };\n+      if (!(internalTokenStream instanceof StringTokenStream)) {\n+        // lazy init the TokenStream as it is heavy to instantiate\n+        // (attributes,...) if not needed (stored field loading)\n+        internalTokenStream = new StringTokenStream();\n+      }\n+      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n+      return internalTokenStream;\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"fd16190940d7495e985f44ce7504562c8bbc91e6": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-4172: clean up redundant throws clauses\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1355069 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-06-28, 9:39 AM",
			"commitName": "fd16190940d7495e985f44ce7504562c8bbc91e6",
			"commitAuthor": "Steven Rowe",
			"commitDateOld": "2012-06-11, 12:26 PM",
			"commitNameOld": "2ac3eb27c41691c7a61b58673be6fe9a5a0eed9e",
			"commitAuthorOld": "Chris M. Hostetter",
			"daysBetweenCommits": 16.88,
			"commitsBetweenForRepo": 110,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,75 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (numericTokenStream == null) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n         // initialize value in TokenStream\n         final Number val = (Number) fieldsData;\n         switch (numericType) {\n         case INT:\n           numericTokenStream.setIntValue(val.intValue());\n           break;\n         case LONG:\n           numericTokenStream.setLongValue(val.longValue());\n           break;\n         case FLOAT:\n           numericTokenStream.setFloatValue(val.floatValue());\n           break;\n         case DOUBLE:\n           numericTokenStream.setDoubleValue(val.doubleValue());\n           break;\n         default:\n           assert false : \"Should never get here\";\n         }\n       } else {\n         // OK -- previously cached and we already updated if\n         // setters were called.\n       }\n \n       return numericTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n-        public boolean incrementToken() throws IOException {\n+        public boolean incrementToken() {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used = true;\n           return true;\n         }\n \n         @Override\n-        public void reset() throws IOException {\n+        public void reset() {\n           used = false;\n         }\n       };\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"854c9ac45223b64acf3e7e4c0a77383a9441268f": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-3777: separate out Int/Long/Float/DoubleField to reduce traps\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1245583 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-02-17, 6:46 AM",
			"commitName": "854c9ac45223b64acf3e7e4c0a77383a9441268f",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2012-02-07, 11:59 AM",
			"commitNameOld": "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c",
			"commitAuthorOld": "Steven Rowe",
			"daysBetweenCommits": 9.78,
			"commitsBetweenForRepo": 124,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,75 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n-    final NumericField.DataType numericType = fieldType().numericType();\n+    final NumericType numericType = fieldType().numericType();\n     if (numericType != null) {\n       if (numericTokenStream == null) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n         // initialize value in TokenStream\n         final Number val = (Number) fieldsData;\n         switch (numericType) {\n         case INT:\n           numericTokenStream.setIntValue(val.intValue());\n           break;\n         case LONG:\n           numericTokenStream.setLongValue(val.longValue());\n           break;\n         case FLOAT:\n           numericTokenStream.setFloatValue(val.floatValue());\n           break;\n         case DOUBLE:\n           numericTokenStream.setDoubleValue(val.doubleValue());\n           break;\n         default:\n           assert false : \"Should never get here\";\n         }\n       } else {\n         // OK -- previously cached and we already updated if\n         // setters were called.\n       }\n \n       return numericTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used = true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used = false;\n         }\n       };\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c": {
			"type": "Yfilerename",
			"commitMessage": "LUCENE-3753: Restructure the Lucene build system\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1241588 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-02-07, 11:59 AM",
			"commitName": "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c",
			"commitAuthor": "Steven Rowe",
			"commitDateOld": "2012-02-07, 10:58 AM",
			"commitNameOld": "8b939cb7d20160f9f8a7baf2030613f0e1e877b4",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 0.04,
			"commitsBetweenForRepo": 1,
			"commitsBetweenForFile": 1,
			"diff": "",
			"extendedDetails": {
				"oldPath": "lucene/src/java/org/apache/lucene/document/Field.java",
				"newPath": "lucene/core/src/java/org/apache/lucene/document/Field.java"
			}
		},
		"9de01b56ebf252ffefe05e606e330a1787b94c9d": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-3453: simplify DocValues/Field API\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1231791 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2012-01-15, 3:05 PM",
			"commitName": "9de01b56ebf252ffefe05e606e330a1787b94c9d",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2011-12-10, 10:17 AM",
			"commitNameOld": "124728c97487911ab225397878e15c5469ba0360",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 36.2,
			"commitsBetweenForRepo": 163,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,43 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n+    final NumericField.DataType numericType = fieldType().numericType();\n+    if (numericType != null) {\n+      if (numericTokenStream == null) {\n+        // lazy init the TokenStream as it is heavy to instantiate\n+        // (attributes,...) if not needed (stored field loading)\n+        numericTokenStream = new NumericTokenStream(type.numericPrecisionStep());\n+        // initialize value in TokenStream\n+        final Number val = (Number) fieldsData;\n+        switch (numericType) {\n+        case INT:\n+          numericTokenStream.setIntValue(val.intValue());\n+          break;\n+        case LONG:\n+          numericTokenStream.setLongValue(val.longValue());\n+          break;\n+        case FLOAT:\n+          numericTokenStream.setFloatValue(val.floatValue());\n+          break;\n+        case DOUBLE:\n+          numericTokenStream.setDoubleValue(val.doubleValue());\n+          break;\n+        default:\n+          assert false : \"Should never get here\";\n+        }\n+      } else {\n+        // OK -- previously cached and we already updated if\n+        // setters were called.\n+      }\n+\n+      return numericTokenStream;\n+    }\n+\n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used = true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used = false;\n         }\n       };\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"67c13bd2fe57d73a824f163f9c73018fa51a1a65": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-3455: Renamed Analyzer.reusableTokenStream to Analyzer.tokenStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1176728 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2011-09-27, 10:26 PM",
			"commitName": "67c13bd2fe57d73a824f163f9c73018fa51a1a65",
			"commitAuthor": "Christopher John Male",
			"commitDateOld": "2011-09-22, 8:16 PM",
			"commitNameOld": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
			"commitAuthorOld": "Christopher John Male",
			"daysBetweenCommits": 5.09,
			"commitsBetweenForRepo": 44,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,43 +1,43 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() == null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used = true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used = false;\n         }\n       };\n     }\n \n     if (tokenStream != null) {\n       return tokenStream;\n     } else if (readerValue() != null) {\n-      return analyzer.reusableTokenStream(name(), readerValue());\n+      return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() != null) {\n-      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"5d4502ad0a3249fec5fcc1e28ce7074f67e8a027": {
			"type": "Ymultichange(Ymovefromfile,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
			"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2011-09-22, 8:16 PM",
			"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
			"commitAuthor": "Christopher John Male",
			"subchanges": [
				{
					"type": "Ymovefromfile",
					"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2011-09-22, 8:16 PM",
					"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
					"commitAuthor": "Christopher John Male",
					"commitDateOld": "2011-09-22, 2:56 PM",
					"commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
					"commitAuthorOld": "Uwe Schindler",
					"daysBetweenCommits": 0.22,
					"commitsBetweenForRepo": 1,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS = new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData != null) {\n-        assert dataType != null;\n-        final Number val = (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() == null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used = true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used = false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream != null) {\n+      return tokenStream;\n+    } else if (readerValue() != null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() != null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {
						"oldPath": "lucene/src/java/org/apache/lucene/document/NumericField.java",
						"newPath": "lucene/src/java/org/apache/lucene/document/Field.java",
						"oldMethodName": "tokenStreamValue",
						"newMethodName": "tokenStream"
					}
				},
				{
					"type": "Yexceptionschange",
					"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2011-09-22, 8:16 PM",
					"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
					"commitAuthor": "Christopher John Male",
					"commitDateOld": "2011-09-22, 2:56 PM",
					"commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
					"commitAuthorOld": "Uwe Schindler",
					"daysBetweenCommits": 0.22,
					"commitsBetweenForRepo": 1,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS = new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData != null) {\n-        assert dataType != null;\n-        final Number val = (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() == null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used = true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used = false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream != null) {\n+      return tokenStream;\n+    } else if (readerValue() != null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() != null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {
						"oldValue": "[]",
						"newValue": "[IOException]"
					}
				},
				{
					"type": "Ybodychange",
					"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2011-09-22, 8:16 PM",
					"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
					"commitAuthor": "Christopher John Male",
					"commitDateOld": "2011-09-22, 2:56 PM",
					"commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
					"commitAuthorOld": "Uwe Schindler",
					"daysBetweenCommits": 0.22,
					"commitsBetweenForRepo": 1,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS = new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData != null) {\n-        assert dataType != null;\n-        final Number val = (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() == null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used = true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used = false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream != null) {\n+      return tokenStream;\n+    } else if (readerValue() != null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() != null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {}
				},
				{
					"type": "Yrename",
					"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2011-09-22, 8:16 PM",
					"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
					"commitAuthor": "Christopher John Male",
					"commitDateOld": "2011-09-22, 2:56 PM",
					"commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
					"commitAuthorOld": "Uwe Schindler",
					"daysBetweenCommits": 0.22,
					"commitsBetweenForRepo": 1,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS = new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData != null) {\n-        assert dataType != null;\n-        final Number val = (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() == null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used = true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used = false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream != null) {\n+      return tokenStream;\n+    } else if (readerValue() != null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() != null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {
						"oldValue": "tokenStreamValue",
						"newValue": "tokenStream"
					}
				},
				{
					"type": "Yparameterchange",
					"commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
					"commitDate": "2011-09-22, 8:16 PM",
					"commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
					"commitAuthor": "Christopher John Male",
					"commitDateOld": "2011-09-22, 2:56 PM",
					"commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
					"commitAuthorOld": "Uwe Schindler",
					"daysBetweenCommits": 0.22,
					"commitsBetweenForRepo": 1,
					"commitsBetweenForFile": 1,
					"diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS = new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData != null) {\n-        assert dataType != null;\n-        final Number val = (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() == null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute = addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used = true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used = false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream != null) {\n+      return tokenStream;\n+    } else if (readerValue() != null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() != null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
					"extendedDetails": {
						"oldValue": "[]",
						"newValue": "[analyzer-Analyzer]"
					}
				}
			]
		},
		"ffb3cbee57e1b075dac827bcc46bc95483c603e0": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-2308: Moved over to using IndexableFieldType interface\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1167668 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2011-09-10, 9:07 PM",
			"commitName": "ffb3cbee57e1b075dac827bcc46bc95483c603e0",
			"commitAuthor": "Christopher John Male",
			"commitDateOld": "2011-08-27, 6:27 AM",
			"commitNameOld": "4dad0ba89f1d663939999be9005433dd629955f1",
			"commitAuthorOld": "Michael McCandless",
			"daysBetweenCommits": 14.61,
			"commitsBetweenForRepo": 64,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,31 +1,31 @@\n   public TokenStream tokenStreamValue() {\n-    if (!indexed()) return null;\n+    if (!type.indexed()) return null;\n     if (numericTS == null) {\n       // lazy init the TokenStream as it is heavy to instantiate\n       // (attributes,...),\n       // if not needed (stored field loading)\n       numericTS = new NumericTokenStream(precisionStep);\n       // initialize value in TokenStream\n       if (fieldsData != null) {\n         assert dataType != null;\n         final Number val = (Number) fieldsData;\n         switch (dataType) {\n           case INT:\n             numericTS.setIntValue(val.intValue());\n             break;\n           case LONG:\n             numericTS.setLongValue(val.longValue());\n             break;\n           case FLOAT:\n             numericTS.setFloatValue(val.floatValue());\n             break;\n           case DOUBLE:\n             numericTS.setDoubleValue(val.doubleValue());\n             break;\n           default:\n             assert false : \"Should never get here\";\n         }\n       }\n     }\n     return numericTS;\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"4dad0ba89f1d663939999be9005433dd629955f1": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-2308: cutover to FieldType\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1162347 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2011-08-27, 6:27 AM",
			"commitName": "4dad0ba89f1d663939999be9005433dd629955f1",
			"commitAuthor": "Michael McCandless",
			"commitDateOld": "2011-07-12, 6:31 AM",
			"commitNameOld": "1c646d24c9f9118e770fca947c2bc5b68775f425",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 46,
			"commitsBetweenForRepo": 260,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,27 +1,31 @@\n-  public TokenStream tokenStreamValue()   {\n-    if (!isIndexed())\n-      return null;\n+  public TokenStream tokenStreamValue() {\n+    if (!indexed()) return null;\n     if (numericTS == null) {\n-      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),\n+      // lazy init the TokenStream as it is heavy to instantiate\n+      // (attributes,...),\n       // if not needed (stored field loading)\n       numericTS = new NumericTokenStream(precisionStep);\n       // initialize value in TokenStream\n       if (fieldsData != null) {\n-        assert type != null;\n+        assert dataType != null;\n         final Number val = (Number) fieldsData;\n-        switch (type) {\n+        switch (dataType) {\n           case INT:\n-            numericTS.setIntValue(val.intValue()); break;\n+            numericTS.setIntValue(val.intValue());\n+            break;\n           case LONG:\n-            numericTS.setLongValue(val.longValue()); break;\n+            numericTS.setLongValue(val.longValue());\n+            break;\n           case FLOAT:\n-            numericTS.setFloatValue(val.floatValue()); break;\n+            numericTS.setFloatValue(val.floatValue());\n+            break;\n           case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue()); break;\n+            numericTS.setDoubleValue(val.doubleValue());\n+            break;\n           default:\n             assert false : \"Should never get here\";\n         }\n       }\n     }\n     return numericTS;\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"400639f54e54ba2cf90b2436652450ded25861f7": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-3065, SOLR-2497: When a NumericField is retrieved from a Document loaded from IndexReader (or IndexSearcher), it will now come back as NumericField. Solr now uses NumericField solely (no more magic).\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1100526 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2011-05-07, 6:14 AM",
			"commitName": "400639f54e54ba2cf90b2436652450ded25861f7",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2011-02-10, 3:50 AM",
			"commitNameOld": "f4e977bb26143619a034574ef4a61e1dd136a3f4",
			"commitAuthorOld": "Uwe Schindler",
			"daysBetweenCommits": 86.06,
			"commitsBetweenForRepo": 535,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,3 +1,27 @@\n   public TokenStream tokenStreamValue()   {\n-    return isIndexed() ? numericTS : null;\n+    if (!isIndexed())\n+      return null;\n+    if (numericTS == null) {\n+      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),\n+      // if not needed (stored field loading)\n+      numericTS = new NumericTokenStream(precisionStep);\n+      // initialize value in TokenStream\n+      if (fieldsData != null) {\n+        assert type != null;\n+        final Number val = (Number) fieldsData;\n+        switch (type) {\n+          case INT:\n+            numericTS.setIntValue(val.intValue()); break;\n+          case LONG:\n+            numericTS.setLongValue(val.longValue()); break;\n+          case FLOAT:\n+            numericTS.setFloatValue(val.floatValue()); break;\n+          case DOUBLE:\n+            numericTS.setDoubleValue(val.doubleValue()); break;\n+          default:\n+            assert false : \"Should never get here\";\n+        }\n+      }\n+    }\n+    return numericTS;\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"778d96752fa94636a2136ea2b4d58a3fcbe283ec": {
			"type": "Yfilerename",
			"commitMessage": "SVN-GIT conversion, path copy emulation.\n",
			"commitDate": "2016-01-22, 4:18 PM",
			"commitName": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
			"commitAuthor": "Dawid Weiss",
			"commitDateOld": "2010-03-17, 7:57 AM",
			"commitNameOld": "2e5c6cdadc820220f8cb86e1b6e215da941649f9",
			"commitAuthorOld": "Uwe Schindler",
			"daysBetweenCommits": 2137.39,
			"commitsBetweenForRepo": 1,
			"commitsBetweenForFile": 1,
			"diff": "",
			"extendedDetails": {
				"oldPath": "src/java/org/apache/lucene/document/NumericField.java",
				"newPath": "lucene/src/java/org/apache/lucene/document/NumericField.java"
			}
		},
		"efb74380fda0da18650dbc66372a7bb1cd41dcf6": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-2285: Code cleanups to remove compiler warnings in eclipse.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@917019 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2010-02-27, 11:14 AM",
			"commitName": "efb74380fda0da18650dbc66372a7bb1cd41dcf6",
			"commitAuthor": "Uwe Schindler",
			"commitDateOld": "2010-01-31, 7:20 AM",
			"commitNameOld": "39b9f97cd414a6fbf7439091e1a42c83f1c71caf",
			"commitAuthorOld": "Robert Muir",
			"daysBetweenCommits": 27.16,
			"commitsBetweenForRepo": 59,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,3 +1,3 @@\n   public TokenStream tokenStreamValue()   {\n-    return isIndexed() ? tokenStream : null;\n+    return isIndexed() ? numericTS : null;\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"edfce675a575bf83baecb52ca58c6e307d50eaad": {
			"type": "Yintroduced",
			"commitMessage": "LUCENE-1701, LUCENE-1687: Add NumericField , make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache, merge ExtendedFieldCache and FieldCache\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@787723 13f79535-47bb-0310-9956-ffa450edef68\n",
			"commitDate": "2009-06-23, 8:42 AM",
			"commitName": "edfce675a575bf83baecb52ca58c6e307d50eaad",
			"commitAuthor": "Uwe Schindler",
			"diff": "@@ -0,0 +1,3 @@\n+  public TokenStream tokenStreamValue()   {\n+    return isIndexed() ? tokenStream : null;\n+  }\n\\ No newline at end of file\n"
		}
	},
	"sha": "38bf976cd4b9e324c21664bd7ae3d554df803705"
}