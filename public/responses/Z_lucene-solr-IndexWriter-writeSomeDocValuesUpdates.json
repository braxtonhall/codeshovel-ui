{
	"repo": "https://github.com/apache/lucene-solr.git",
	"file": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java",
	"method": {
		"longName": "void writeSomeDocValuesUpdates();",
		"startLine": 579,
		"methodName": "writeSomeDocValuesUpdates",
		"isStatic": false,
		"isAbstract": false,
		"visibility": ""
	},
	"history": {
		"772e171ac6e70c96295f65749d0d15339133b8a6": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-8358: Relax assertion in IW#writeSomeDocValuesUpdates\n\nThis assertion is too strict since we can see this situation if for instance\na ReadersAndUpdates instance gets written to disk concurrently and\nreaderpooling is off. This change also simplifies ReaderPool#getReadersByRam and\nadds a test for it.\n",
			"commitDate": "2018-06-15, 3:01 AM",
			"commitName": "772e171ac6e70c96295f65749d0d15339133b8a6",
			"commitAuthor": "Simon Willnauer",
			"commitDateOld": "2018-06-13, 1:10 AM",
			"commitNameOld": "61e68ec1e8cd409cb51a209f827fc64710b31f6f",
			"commitAuthorOld": "Simon Willnauer",
			"daysBetweenCommits": 2.08,
			"commitsBetweenForRepo": 19,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,57 +1,59 @@\n   void writeSomeDocValuesUpdates() throws IOException {\n     if (writeDocValuesLock.compareAndSet(false, true)) {\n       try {\n         final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n         // If the reader pool is > 50% of our IW buffer, then write the updates:\n         if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n           long startNS = System.nanoTime();\n \n           long ramBytesUsed = getReaderPoolRamBytesUsed();\n           if (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n             if (infoStream.isEnabled(\"BD\")) {\n               infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n                   ramBytesUsed/1024./1024., ramBufferSizeMB));\n             }\n \n             // Sort by largest ramBytesUsed:\n-            PriorityQueue<ReadersAndUpdates> queue = readerPool.getReadersByRam();\n+            final List<ReadersAndUpdates> list = readerPool.getReadersByRam();\n             int count = 0;\n-            while (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n-              ReadersAndUpdates rld = queue.poll();\n-              if (rld == null) {\n+            for (ReadersAndUpdates rld : list) {\n+\n+              if (ramBytesUsed <= 0.5 * ramBufferSizeMB * 1024 * 1024) {\n                 break;\n               }\n-\n               // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n               // not all of those bytes can be written here:\n               long bytesUsedBefore = rld.ramBytesUsed.get();\n-\n+              if (bytesUsedBefore == 0) {\n+                continue; // nothing to do here - lets not acquire the lock\n+              }\n               // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n               // other threads get a chance to run in between our writes.\n               synchronized (this) {\n                 // It's possible that the segment of a reader returned by readerPool#getReadersByRam\n                 // is dropped before being processed here. If it happens, we need to skip that reader.\n+                // this is also best effort to free ram, there might be some other thread writing this rld concurrently\n+                // which wins and then if readerPooling is off this rld will be dropped.\n                 if (readerPool.get(rld.info, false) == null) {\n-                  assert segmentInfos.contains(rld.info) == false : \"Segment [\" + rld.info + \"] is not dropped yet\";\n                   continue;\n                 }\n                 if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n                   checkpointNoSIS();\n                 }\n               }\n               long bytesUsedAfter = rld.ramBytesUsed.get();\n               ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n               count++;\n             }\n \n             if (infoStream.isEnabled(\"BD\")) {\n               infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n                   count, getReaderPoolRamBytesUsed()/1024./1024., ramBufferSizeMB, ((System.nanoTime() - startNS)/1000000000.)));\n             }\n           }\n         }\n       } finally {\n         writeDocValuesLock.set(false);\n       }\n     }\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"61e68ec1e8cd409cb51a209f827fc64710b31f6f": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-8355: Prevent IW from opening an already dropped segment while DV updates are written\n\nThis change fixes an isse where IW asks ReadersAndUpdates to write a DV updates for a\nsegment that has been dropped concurrently. The race only occurs if ram buffers are filled\nup enough to trigger flushing DV to disk.\n\nCo-authored-by: Nhat Nguyen <nhat.nguyen@elastic.co>\n",
			"commitDate": "2018-06-13, 1:10 AM",
			"commitName": "61e68ec1e8cd409cb51a209f827fc64710b31f6f",
			"commitAuthor": "Simon Willnauer",
			"commitDateOld": "2018-06-04, 6:05 AM",
			"commitNameOld": "fe83838ec3768f25964a04510cd10772cf034d34",
			"commitAuthorOld": "Simon Willnauer",
			"daysBetweenCommits": 8.8,
			"commitsBetweenForRepo": 75,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,51 +1,57 @@\n   void writeSomeDocValuesUpdates() throws IOException {\n     if (writeDocValuesLock.compareAndSet(false, true)) {\n       try {\n         final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n         // If the reader pool is > 50% of our IW buffer, then write the updates:\n         if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n           long startNS = System.nanoTime();\n \n           long ramBytesUsed = getReaderPoolRamBytesUsed();\n           if (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n             if (infoStream.isEnabled(\"BD\")) {\n               infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n                   ramBytesUsed/1024./1024., ramBufferSizeMB));\n             }\n \n             // Sort by largest ramBytesUsed:\n             PriorityQueue<ReadersAndUpdates> queue = readerPool.getReadersByRam();\n             int count = 0;\n             while (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n               ReadersAndUpdates rld = queue.poll();\n               if (rld == null) {\n                 break;\n               }\n \n               // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n               // not all of those bytes can be written here:\n               long bytesUsedBefore = rld.ramBytesUsed.get();\n \n               // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n               // other threads get a chance to run in between our writes.\n               synchronized (this) {\n+                // It's possible that the segment of a reader returned by readerPool#getReadersByRam\n+                // is dropped before being processed here. If it happens, we need to skip that reader.\n+                if (readerPool.get(rld.info, false) == null) {\n+                  assert segmentInfos.contains(rld.info) == false : \"Segment [\" + rld.info + \"] is not dropped yet\";\n+                  continue;\n+                }\n                 if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n                   checkpointNoSIS();\n                 }\n               }\n               long bytesUsedAfter = rld.ramBytesUsed.get();\n               ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n               count++;\n             }\n \n             if (infoStream.isEnabled(\"BD\")) {\n               infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n                   count, getReaderPoolRamBytesUsed()/1024./1024., ramBufferSizeMB, ((System.nanoTime() - startNS)/1000000000.)));\n             }\n           }\n         }\n       } finally {\n         writeDocValuesLock.set(false);\n       }\n     }\n   }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"8975692953713923bd1cc67766cf92565183c2b8": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-8260: Extract ReaderPool from IndexWriter\n\nReaderPool plays a central role in the IndexWriter pooling NRT readers\r\nand making sure we write buffered deletes and updates to disk. This class\r\nused to be a non-static inner class accessing many aspects including locks\r\nfrom the IndexWriter itself. This change moves the class outside of IW and\r\ndefines it's responsibility in a clear way with respect to locks etc. Now\r\nIndexWriter doesn't need to share ReaderPool anymore and reacts on writes done\r\ninside the pool by checkpointing internally. This also removes acquiring the IW\r\nlock inside the reader pool which makes reasoning about concurrency difficult.\r\n\r\nThis change also add javadocs and dedicated tests for the ReaderPool class.\r\n",
			"commitDate": "2018-04-23, 1:29 AM",
			"commitName": "8975692953713923bd1cc67766cf92565183c2b8",
			"commitAuthor": "Simon Willnauer",
			"commitDateOld": "2018-04-17, 7:26 AM",
			"commitNameOld": "d904112428184ce9c1726313add5d184f4014a72",
			"commitAuthorOld": "Simon Willnauer",
			"daysBetweenCommits": 5.75,
			"commitsBetweenForRepo": 31,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,62 +1,51 @@\n-    void writeSomeDocValuesUpdates() throws IOException {\n+  void writeSomeDocValuesUpdates() throws IOException {\n+    if (writeDocValuesLock.compareAndSet(false, true)) {\n+      try {\n+        final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n+        // If the reader pool is > 50% of our IW buffer, then write the updates:\n+        if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n+          long startNS = System.nanoTime();\n \n-      assert Thread.holdsLock(IndexWriter.this) == false;\n+          long ramBytesUsed = getReaderPoolRamBytesUsed();\n+          if (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n+            if (infoStream.isEnabled(\"BD\")) {\n+              infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n+                  ramBytesUsed/1024./1024., ramBufferSizeMB));\n+            }\n \n-      if (writeDocValuesLock.compareAndSet(false, true)) {\n-        try {\n-\n-          LiveIndexWriterConfig config = getConfig();\n-          double mb = config.getRAMBufferSizeMB();\n-          // If the reader pool is > 50% of our IW buffer, then write the updates:\n-          if (mb != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n-            long startNS = System.nanoTime();\n-            \n-            long ramBytesUsed = ramBytesUsed();\n-            if (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n-              if (infoStream.isEnabled(\"BD\")) {\n-                infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n-                                                       ramBytesUsed/1024./1024., mb));\n+            // Sort by largest ramBytesUsed:\n+            PriorityQueue<ReadersAndUpdates> queue = readerPool.getReadersByRam();\n+            int count = 0;\n+            while (ramBytesUsed > 0.5 * ramBufferSizeMB * 1024 * 1024) {\n+              ReadersAndUpdates rld = queue.poll();\n+              if (rld == null) {\n+                break;\n               }\n-          \n-              // Sort by largest ramBytesUsed:\n-              PriorityQueue<ReadersAndUpdates> queue = new PriorityQueue<>(readerMap.size(), (a, b) -> Long.compare(b.ramBytesUsed.get(), a.ramBytesUsed.get()));\n+\n+              // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n+              // not all of those bytes can be written here:\n+              long bytesUsedBefore = rld.ramBytesUsed.get();\n+\n+              // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n+              // other threads get a chance to run in between our writes.\n               synchronized (this) {\n-                for (ReadersAndUpdates rld : readerMap.values()) {\n-                  queue.add(rld);\n+                if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n+                  checkpointNoSIS();\n                 }\n               }\n+              long bytesUsedAfter = rld.ramBytesUsed.get();\n+              ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n+              count++;\n+            }\n \n-              int count = 0;\n-              while (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n-                ReadersAndUpdates rld = queue.poll();\n-                if (rld == null) {\n-                  break;\n-                }\n-\n-                // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n-                // not all of those bytes can be written here:\n-                long bytesUsedBefore = rld.ramBytesUsed.get();\n-\n-                // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n-                // other threads get a chance to run in between our writes.\n-                synchronized (IndexWriter.this) {\n-                  if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n-                    checkpointNoSIS();\n-                  }\n-                }\n-                long bytesUsedAfter = rld.ramBytesUsed.get();\n-                ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n-                count++;\n-              }\n-\n-              if (infoStream.isEnabled(\"BD\")) {\n-                infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n-                                                       count, ramBytesUsed()/1024./1024., mb, ((System.nanoTime() - startNS)/1000000000.)));\n-              }\n+            if (infoStream.isEnabled(\"BD\")) {\n+              infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n+                  count, getReaderPoolRamBytesUsed()/1024./1024., ramBufferSizeMB, ((System.nanoTime() - startNS)/1000000000.)));\n             }\n           }\n-        } finally {\n-          writeDocValuesLock.set(false);\n         }\n+      } finally {\n+        writeDocValuesLock.set(false);\n       }\n-    }\n\\ No newline at end of file\n+    }\n+  }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"acb3c379427193036f3d56503529400736ac5dff": {
			"type": "Ybodychange",
			"commitMessage": "LUCENE-8232: Separate out PendingDeletes from ReadersAndUpdates\n\nToday ReadersAndUpdates is tightly coupled with IW and all the\nhandling of pending deletes. This change decouples IW and pending\ndeletes from ReadersAndUpdates and makes PendingDeletes unittestable.\n",
			"commitDate": "2018-03-31, 1:25 AM",
			"commitName": "acb3c379427193036f3d56503529400736ac5dff",
			"commitAuthor": "Simon Willnauer",
			"commitDateOld": "2018-03-21, 1:41 AM",
			"commitNameOld": "d4e69c5cd868d0f5b71da0f4b23c2cd61d1b0ea0",
			"commitAuthorOld": "Simon Willnauer",
			"daysBetweenCommits": 9.99,
			"commitsBetweenForRepo": 81,
			"commitsBetweenForFile": 1,
			"diff": "@@ -1,60 +1,62 @@\n     void writeSomeDocValuesUpdates() throws IOException {\n \n       assert Thread.holdsLock(IndexWriter.this) == false;\n \n       if (writeDocValuesLock.compareAndSet(false, true)) {\n         try {\n \n           LiveIndexWriterConfig config = getConfig();\n           double mb = config.getRAMBufferSizeMB();\n           // If the reader pool is > 50% of our IW buffer, then write the updates:\n           if (mb != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n             long startNS = System.nanoTime();\n             \n             long ramBytesUsed = ramBytesUsed();\n             if (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n               if (infoStream.isEnabled(\"BD\")) {\n                 infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n                                                        ramBytesUsed/1024./1024., mb));\n               }\n           \n               // Sort by largest ramBytesUsed:\n               PriorityQueue<ReadersAndUpdates> queue = new PriorityQueue<>(readerMap.size(), (a, b) -> Long.compare(b.ramBytesUsed.get(), a.ramBytesUsed.get()));\n               synchronized (this) {\n                 for (ReadersAndUpdates rld : readerMap.values()) {\n                   queue.add(rld);\n                 }\n               }\n \n               int count = 0;\n               while (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n                 ReadersAndUpdates rld = queue.poll();\n                 if (rld == null) {\n                   break;\n                 }\n \n                 // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n                 // not all of those bytes can be written here:\n                 long bytesUsedBefore = rld.ramBytesUsed.get();\n \n                 // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n                 // other threads get a chance to run in between our writes.\n                 synchronized (IndexWriter.this) {\n-                  rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n+                  if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n+                    checkpointNoSIS();\n+                  }\n                 }\n                 long bytesUsedAfter = rld.ramBytesUsed.get();\n                 ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n                 count++;\n               }\n \n               if (infoStream.isEnabled(\"BD\")) {\n                 infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n                                                        count, ramBytesUsed()/1024./1024., mb, ((System.nanoTime() - startNS)/1000000000.)));\n               }\n             }\n           }\n         } finally {\n           writeDocValuesLock.set(false);\n         }\n       }\n     }\n\\ No newline at end of file\n",
			"extendedDetails": {}
		},
		"58105a203a19d18a56e09cf69dc0083c1b890315": {
			"type": "Yintroduced",
			"commitMessage": "LUCENE-7868: use multiple threads to concurrently resolve deletes and DV udpates\n",
			"commitDate": "2017-06-21, 10:47 AM",
			"commitName": "58105a203a19d18a56e09cf69dc0083c1b890315",
			"commitAuthor": "Mike McCandless",
			"diff": "@@ -0,0 +1,60 @@\n+    void writeSomeDocValuesUpdates() throws IOException {\n+\n+      assert Thread.holdsLock(IndexWriter.this) == false;\n+\n+      if (writeDocValuesLock.compareAndSet(false, true)) {\n+        try {\n+\n+          LiveIndexWriterConfig config = getConfig();\n+          double mb = config.getRAMBufferSizeMB();\n+          // If the reader pool is > 50% of our IW buffer, then write the updates:\n+          if (mb != IndexWriterConfig.DISABLE_AUTO_FLUSH) {\n+            long startNS = System.nanoTime();\n+            \n+            long ramBytesUsed = ramBytesUsed();\n+            if (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n+              if (infoStream.isEnabled(\"BD\")) {\n+                infoStream.message(\"BD\", String.format(Locale.ROOT, \"now write some pending DV updates: %.2f MB used vs IWC Buffer %.2f MB\",\n+                                                       ramBytesUsed/1024./1024., mb));\n+              }\n+          \n+              // Sort by largest ramBytesUsed:\n+              PriorityQueue<ReadersAndUpdates> queue = new PriorityQueue<>(readerMap.size(), (a, b) -> Long.compare(b.ramBytesUsed.get(), a.ramBytesUsed.get()));\n+              synchronized (this) {\n+                for (ReadersAndUpdates rld : readerMap.values()) {\n+                  queue.add(rld);\n+                }\n+              }\n+\n+              int count = 0;\n+              while (ramBytesUsed > 0.5 * mb * 1024 * 1024) {\n+                ReadersAndUpdates rld = queue.poll();\n+                if (rld == null) {\n+                  break;\n+                }\n+\n+                // We need to do before/after because not all RAM in this RAU is used by DV updates, and\n+                // not all of those bytes can be written here:\n+                long bytesUsedBefore = rld.ramBytesUsed.get();\n+\n+                // Only acquire IW lock on each write, since this is a time consuming operation.  This way\n+                // other threads get a chance to run in between our writes.\n+                synchronized (IndexWriter.this) {\n+                  rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n+                }\n+                long bytesUsedAfter = rld.ramBytesUsed.get();\n+                ramBytesUsed -= bytesUsedBefore - bytesUsedAfter;\n+                count++;\n+              }\n+\n+              if (infoStream.isEnabled(\"BD\")) {\n+                infoStream.message(\"BD\", String.format(Locale.ROOT, \"done write some DV updates for %d segments: now %.2f MB used vs IWC Buffer %.2f MB; took %.2f sec\",\n+                                                       count, ramBytesUsed()/1024./1024., mb, ((System.nanoTime() - startNS)/1000000000.)));\n+              }\n+            }\n+          }\n+        } finally {\n+          writeDocValuesLock.set(false);\n+        }\n+      }\n+    }\n\\ No newline at end of file\n"
		}
	},
	"sha": "38bf976cd4b9e324c21664bd7ae3d554df803705"
}